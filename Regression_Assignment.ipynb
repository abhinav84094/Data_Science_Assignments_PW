{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Simple Linear Regression?\n",
        "- Simple Linear Regression is a supervised learning technique that models the relationship between one independent variable and one dependent variable using a straight line.\n",
        "- Key Characteristics\n",
        "  - Uses only one feature\n",
        "  - Assumes a linear relationship\n",
        "  - Output is continuous\n",
        "- Simple Linear Regression models the relationship between one input variable and one output variable using a straight line."
      ],
      "metadata": {
        "id": "Ekg4FQ6DcEjN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What are the key assumptions of Simple Linear Regression?\n",
        "\n",
        "Simple Linear Regression works correctly only if these assumptions are satisfied:\n",
        "- Linearity\n",
        "  - The relationship between X (independent) and Y (dependent) is linear.\n",
        "  - Change in Y is proportional to change in X.\n",
        "- Independence\n",
        "  - Observations are independent of each other.\n",
        "  - No correlation between residuals.\n",
        "- Homoscedasticity\n",
        "  - The variance of errors is constant across all values of X.\n",
        "  - Errors are evenly spread.\n",
        "- Normality of Errors\n",
        "  - Residuals (errors) are normally distributed.\n",
        "  - Mainly required for hypothesis testing.\n",
        "- No Outliers (Implicit)\n",
        "  - Extreme values should not heavily influence the model.\n",
        "  - Outliers can distort slope and intercept.\n",
        "-\n",
        "  | Assumption       | Meaning                           |\n",
        "  | ---------------- | --------------------------------- |\n",
        "  | Linearity        | Straight-line relationship        |\n",
        "  | Independence     | Observations not related          |\n",
        "  | Homoscedasticity | Constant error variance           |\n",
        "  | Normality        | Errors follow normal distribution |\n",
        "\n",
        "- Simple Linear Regression assumes linearity, independence, homoscedasticity, and normal distribution of errors."
      ],
      "metadata": {
        "id": "8_01TkGAcld9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What does the coefficient m represent in the equation Y=mX+c?\n",
        "- The coefficient m represents the slope of the regression line.\n",
        "- It shows the rate of change of Y with respect to X.\n",
        "- For every 1 unit increase in X, Y changes by m units.\n",
        "- Interpretation\n",
        "  - m > 0 â†’ Positive relationship (Y increases as X increases)\n",
        "  - m < 0 â†’ Negative relationship (Y decreases as X increases)\n",
        "  - m = 0 â†’ No linear relationship"
      ],
      "metadata": {
        "id": "U85KIdmddto_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What does the intercept c represent in the equation Y=mX+c?\n",
        "- The intercept c represents the value of Y when X = 0.\n",
        "- It is the starting point of the regression line.\n",
        "- It shows where the line cuts the Y-axis.\n",
        "- Interpretation\n",
        "  - If c > 0 â†’ Y has a positive baseline value when X is zero.\n",
        "  - If c < 0 â†’ Y starts at a negative value when X is zero.\n",
        "  - If c = 0 â†’ The line passes through the origin."
      ],
      "metadata": {
        "id": "FXVipD_VeOnk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. How do we calculate the slope m in Simple Linear Regression?\n",
        "- In Simple Linear Regression, the slope m is calculated using the least squares method.\n",
        "- Formula for Slope m\n",
        "  - m = âˆ‘(xiâ€‹âˆ’xË‰)(yiâ€‹âˆ’yË‰â€‹) / âˆ‘(xiâ€‹âˆ’xË‰)2â€‹\n",
        "- The slope m is calculated by minimizing the sum of squared errors using the least squares formula relating X and Y deviations from their means.\n"
      ],
      "metadata": {
        "id": "XcnDIe7Ee2k-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. What is the purpose of the least squares method in Simple Linear Regression?\n",
        "- The least squares method is used to find the best-fit line by minimizing prediction error.\n"
      ],
      "metadata": {
        "id": "2pNQWS8qgHPX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. How is the coefficient of determination (RÂ²) interpreted in Simple Linear Regression?\n",
        "- The coefficient of determination (RÂ²) measures how well the regression model explains the variability in the dependent variable (Y).\n",
        "- Meaning of RÂ²\n",
        "  - RÂ² indicates the proportion of variance in Y explained by X.\n",
        "  - Its value lies between 0 and 1.\n",
        "    - RÂ² =  Explained Variance / Total Variance\n",
        "\tâ€‹\n",
        "- | RÂ² Value | Interpretation                        |\n",
        "| -------- | ------------------------------------- |\n",
        "| 0        | X explains none of the variation in Y |\n",
        "| 0.3      | X explains 30% of the variation       |\n",
        "| 0.7      | X explains 70% of the variation       |\n",
        "| 1        | Perfect linear relationship           |\n",
        "\n",
        "- RÂ² represents the proportion of variance in the dependent variable that is explained by the independent variable in the regression model."
      ],
      "metadata": {
        "id": "NERe4IVOglLs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. What is Multiple Linear Regression?\n",
        "- Multiple Linear Regression (MLR) is a supervised learning technique that models the relationship between one dependent variable and two or more independent variables using a linear equation.\n",
        "- Multiple Linear Regression models the relationship between one dependent variable and multiple independent variables using a linear equation."
      ],
      "metadata": {
        "id": "S7nldAumiGCe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. What is the main difference between Simple and Multiple Linear Regression?\n",
        "- The main difference lies in the number of independent variables used to predict the dependent variable.\n",
        "- | Aspect                          | Simple Linear Regression | Multiple Linear Regression            |\n",
        "| ------------------------------- | ------------------------ | ------------------------------------- |\n",
        "| Number of independent variables | **One (X)**              | **Two or more (Xâ‚, Xâ‚‚, â€¦)**           |\n",
        "| Equation                        | ( Y = mX + c )           | ( Y = b_0 + b_1X_1 + b_2X_2 + \\dots ) |\n",
        "| Complexity                      | Simpler                  | More complex                          |\n",
        "| Interpretation                  | Easy                     | More detailed                         |\n",
        "\n",
        "- Example\n",
        "  - Simple Linear Regression:\n",
        "    - Predict salary using years of experience\n",
        "  - Multiple Linear Regression:\n",
        "    - Predict salary using experience, education, skills\n",
        "\n",
        "- Simple Linear Regression uses one independent variable, while Multiple Linear Regression uses two or more independent variables to predict the dependent variable.\n"
      ],
      "metadata": {
        "id": "6QycciEWi7dP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. What are the key assumptions of Multiple Linear Regression?\n",
        "\n",
        "Multiple Linear Regression (MLR) produces reliable results only when these assumptions hold:\n",
        "- Linearity\n",
        "  - The relationship between each independent variable and the dependent variable is linear.\n",
        "  - Effects are additive.\n",
        "- Independence of Errors\n",
        "  - Residuals are independent of each other.\n",
        "  - No autocorrelation.\n",
        "- Homoscedasticity\n",
        "  - Error variance is constant across all levels of predictors.\n",
        "  - No funnel-shaped residual patterns.\n",
        "- Normality of Errors\n",
        "  - Residuals are normally distributed.\n",
        "  - Required mainly for hypothesis testing and confidence intervals.\n",
        "- No Multicollinearity\n",
        "  - Independent variables should not be highly correlated with each other.\n",
        "  - High multicollinearity inflates coefficient variances.\n",
        "- No Significant Outliers or Influential Points\n",
        "  - Extreme values should not dominate the model."
      ],
      "metadata": {
        "id": "20pWoXqdjkVf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model.\n",
        "- Heteroscedasticity occurs when the variance of the error terms (residuals) is not constant across all levels of the independent variables in a Multiple Linear Regression (MLR) model.\n",
        "- | Component                       | Effect                           |\n",
        "| ------------------------------- | -------------------------------- |\n",
        "| **Regression coefficients (Î²)** | Unbiased and consistent          |\n",
        "| **Standard errors**             | Incorrect (often underestimated) |\n",
        "| **t-tests / F-tests**           | Invalid                          |\n",
        "| **p-values**                    | Misleading                       |\n",
        "| **Confidence intervals**        | Too narrow or too wide           |\n",
        "| **Model efficiency**            | Reduced                          |\n",
        "\n",
        "- Heteroscedasticity violates the constant-variance assumption in Multiple Linear Regression, leading to unreliable standard errors, p-values, and hypothesis tests, though coefficient estimates remain unbiased."
      ],
      "metadata": {
        "id": "vCcgO6ab1Stu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. How can you improve a Multiple Linear Regression model with high multicollinearity.\n",
        "- Multicollinearity occurs when two or more independent variables in a Multiple Linear Regression (MLR) model are highly correlated, making it difficult to isolate their individual effects.\n",
        "- Why It Is a Problem\n",
        "  - Regression coefficients become unstable\n",
        "  - Standard errors increase\n",
        "  - t-statistics decrease (variables appear insignificant)\n",
        "  - Small data changes cause large coefficient changes\n",
        "  - Interpretation of coefficients becomes unreliable\n",
        "- Multicollinearity can be handled by removing or combining correlated predictors, applying PCA, or using regularization methods like Ridge Regression to stabilize coefficient estimates.\n"
      ],
      "metadata": {
        "id": "hY-hKn5XbvIv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. What are some common techniques for transforming categorical variables for use in regression models.\n",
        "- Regression models require numerical inputs, so categorical variables must be converted into numeric form. Below are the most common and widely accepted techniques.\n",
        "- | Technique | Suitable For     | Pros          | Cons                    |\n",
        "| --------- | ---------------- | ------------- | ----------------------- |\n",
        "| One-hot   | Nominal          | Interpretable | High dimensionality     |\n",
        "| Label     | Ordinal          | Simple        | Misleading if unordered |\n",
        "| Ordinal   | Ordered          | Meaningful    | Requires correct order  |\n",
        "| Target    | High-cardinality | Powerful      | Risk of leakage         |\n",
        "| Frequency | High-cardinality | Simple        | Loses category meaning  |\n",
        "| Binary    | Many categories  | Compact       | Less interpretable      |\n",
        "| Hashing   | Huge datasets    | Fast          | Collisions              |\n",
        "\n",
        "- Categorical variables are transformed for regression using techniques such as one-hot encoding, ordinal encoding, target encoding, and hashingâ€”chosen based on whether the categories are ordered, their cardinality, and the need for interpretability."
      ],
      "metadata": {
        "id": "-6FL0P0CeecN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. What is the role of interaction terms in Multiple Linear Regression.\n",
        "- An interaction term in Multiple Linear Regression (MLR) is used to capture the situation where the effect of one independent variable on the dependent variable depends on the value of another independent variable.\n",
        "- Interaction terms allow a Multiple Linear Regression model to represent situations where the effect of one predictor on the response variable changes depending on another predictor.\n"
      ],
      "metadata": {
        "id": "0LbGY767gNHs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. How can the interpretation of intercept differ between Simple and Multiple Linear Regression?\n",
        "\n",
        "Interpretation of the Intercept: Simple vs. Multiple Linear Regression\n",
        "- Simple Linear Regression (SLR)\n",
        "  - Y=Î²0â€‹+Î²1â€‹X+Îµ\n",
        "  - It represents the baseline level of the dependent variable in the absence of the predictor.\n",
        "- Multiple Linear Regression (MLR)\n",
        "  - Y=Î²0â€‹+Î²1â€‹X1â€‹+Î²2â€‹X2â€‹+â‹¯+Î²kâ€‹Xkâ€‹+Îµ\n",
        "  - It represents the baseline outcome when every explanatory variable is at its reference or zero level.\n",
        "- | Aspect                  | Simple Linear Regression           | Multiple Linear Regression           |\n",
        "| ----------------------- | ---------------------------------- | ------------------------------------ |\n",
        "| Condition for intercept | (X = 0)                            | All (X_1, X_2, \\ldots, X_k = 0)      |\n",
        "| Ease of interpretation  | Usually simpler                    | Often complex or unrealistic         |\n",
        "| Practical meaning       | Often meaningful if (X=0) is valid | Frequently just a baseline/reference |\n",
        "| Sensitivity to scaling  | Lower                              | Higher                               |\n",
        "\n",
        "\n",
        "- Centering predictors (subtracting their means) makes the intercept represent the expected value of\n",
        "ð‘Œ\n",
        "Y at average predictor values, which is often more interpretableâ€”especially in Multiple Linear Regression.\n"
      ],
      "metadata": {
        "id": "21rR8EGJ1mSy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. What is the significance of the slope in regression analysis, and how does it affect predictions.\n",
        "- In a regression model, the slope coefficient measures the rate of change in the dependent variable\n",
        "ð‘Œ\n",
        "Y for a one-unit change in an independent variable\n",
        "ð‘‹\n",
        "X, holding other variables constant (in Multiple Linear Regression).\n",
        "- Simple Linear Regression:\n",
        "  - Y=Î²0â€‹+Î²1â€‹X+Îµ\n",
        "  - Î²\n",
        "i\n",
        ": expected change in\n",
        "ð‘Œ\n",
        "Y for a 1-unit increase in\n",
        "ð‘‹\n",
        "ð‘–\n",
        "X\n",
        "i\n",
        ", holding all other predictors constant.\n",
        "- Multiple Linear Regression:\n",
        "  - Y=Î²0â€‹+Î²1â€‹X1â€‹+Î²2â€‹X2â€‹+â‹¯+Î²kâ€‹Xkâ€‹+Îµ\n",
        "  - ð›½\n",
        "ð‘–\n",
        "Î²\n",
        "i\n",
        ": expected change in\n",
        "ð‘Œ\n",
        "Y for a 1-unit increase in\n",
        "ð‘‹\n",
        "ð‘–\n",
        "X\n",
        "i\n",
        ", holding all other predictors constant.\n",
        "\n",
        "- Statistical Significance of the Slope\n",
        "  - The slopeâ€™s statistical significance indicates whether the relationship between\n",
        "ð‘‹\n",
        "X and\n",
        "ð‘Œ\n",
        "Y is likely to be real rather than due to random chance.\n",
        "\n",
        "\n",
        "- The slope quantifies direction and strength of the relationship between predictors and the outcome.\n",
        "\n",
        "- It determines how predictions change when input variables change.\n",
        "\n",
        "- Its statistical significance indicates whether the predictor meaningfully contributes to the model."
      ],
      "metadata": {
        "id": "FPx9TdLn205T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. How does the intercept in a regression model provide context for the relationship between variables?\n",
        "- The intercept defines the starting level of the dependent variable.\n",
        "\n",
        "- It provides context by setting a baseline for interpreting slopes.\n",
        "\n",
        "- Its practical meaning depends on how predictors are defined and scaled."
      ],
      "metadata": {
        "id": "fEZC96sH39H9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. What are the limitations of using RÂ² as a sole measure of model performance?\n",
        "\n",
        "Limitations of Using\n",
        " RÂ²\n",
        " as the Sole Measure of Model Performance\n",
        "- Does Not Indicate Causality or Correct Model Form\n",
        "  - A high\n",
        " RÂ²\n",
        " only means the model explains a large proportion of variance in\n",
        "ð‘Œ\n",
        "Y.\n",
        "- Always Increases with More Predictors\n",
        "  - In Multiple Linear Regression,\n",
        " RÂ²\n",
        " never decreases when additional variables are added.\n",
        "  - This can reward overfitting, even if new predictors have no real explanatory power.\n",
        "- Sensitive to the Variability of the Dependent Variable\n",
        "  - RÂ² depends on how much variation exists in\n",
        "ð‘Œ\n",
        "Y.\n",
        "  - With low natural variability in\n",
        "ð‘Œ\n",
        "Y, even good models may have a low\n",
        "RÂ²\n",
        ".\n",
        "  - With highly variable data, a weak model can still show a high\n",
        "RÂ²\n",
        ".\n",
        "- Does Not Measure Predictive Accuracy\n",
        "  - reflects in-sample fit, not how well the model predicts new data.\n",
        "  - A model with high RÂ² can perform poorly on unseen data.\n",
        "- Not Comparable Across Different Datasets or Outcomes\n",
        "- | Limitation            | Why It Matters                          |\n",
        "| --------------------- | --------------------------------------- |\n",
        "| No causality          | High fit â‰  meaningful relationship      |\n",
        "| Rewards complexity    | Encourages overfitting                  |\n",
        "| Scale-dependent       | Influenced by variance in (Y)           |\n",
        "| No predictive insight | Poor out-of-sample performance possible |\n",
        "| Assumption-blind      | Hides diagnostic problems               |\n"
      ],
      "metadata": {
        "id": "3Ts6_ME34VSI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. How would you interpret a large standard error for a regression coefficient?\n",
        "\n",
        "Interpretation of a Large Standard Error for a Regression Coefficient\n",
        "- What the Standard Error Represents\n",
        "  - The standard error (SE) of a regression coefficient measures the uncertainty or imprecision in the estimated coefficient.\n",
        "  - Small SE â†’ estimate is precise and stable\n",
        "  - Large SE â†’ estimate is imprecise and unstable\n",
        "- Meaning of a Large Standard Error\n",
        "  - A large standard error indicates that the estimated effect of the predictor is not well determined by the data.\n",
        "- A large standard error means low precision in the coefficient estimate.\n",
        "- It weakens statistical significance and practical interpretability.\n",
        "- It signals potential data or model issues that should be investigated."
      ],
      "metadata": {
        "id": "6T68zUIQ5evb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. How can heteroscedasticity be identified in residual plots, and why is it important to address it.\n",
        "\n",
        "Identifying Heteroscedasticity in Residual Plots and Why It Matters\n",
        "- What Heteroscedasticity Means\n",
        "  - Heteroscedasticity occurs when the variance of the error terms is not constant across levels of the predicted values or an independent variable.\n",
        "- How to Identify Heteroscedasticity Using Residual Plots\n",
        "  - Residuals vs. Fitted Values Plot\n",
        "This is the most common diagnostic.\n",
        "  - Funnel or cone shape: residual spread increases or decreases with fitted values.\n",
        "  - Fan pattern: variance expands as predictions grow.\n",
        "  - Systematic widening or narrowing of residuals.\n",
        "\n",
        "- Identification: Look for non-constant spread (fan/funnel patterns) in residual plots.\n",
        "\n",
        "- Importance: Heteroscedasticity distorts standard errors and hypothesis tests.\n",
        "\n",
        "- Action: Always diagnose and correct for it to ensure valid statistical inference."
      ],
      "metadata": {
        "id": "r9GOuk6W6BPc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. What does it mean if a Multiple Linear Regression model has a high RÂ² but low adjusted RÂ²?\n",
        "- High\n",
        "RÂ² + low adjusted\n",
        "RÂ²\n",
        " signals overfitting.\n",
        "\n",
        "- The model explains variance but lacks parsimony.\n",
        "\n",
        "- Adjusted\n",
        "RÂ²\n",
        " provides a more realistic assessment of model quality in Multiple Linear Regression."
      ],
      "metadata": {
        "id": "TuoJIh2e6sJD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. Why is it important to scale variables in Multiple Linear Regression?\n",
        "- Scaling refers to transforming variables so they are on a comparable numerical scale\n",
        "(e.g., standardization to mean 0 and standard deviation 1, or normalization to a fixed range).\n",
        "- Scaling improves numerical stability, interpretability, and comparability of coefficients.\n",
        "- It is especially important when predictors differ widely in units or when using interaction, polynomial, or regularized models.\n",
        "- While not always required for OLS, it is often best practice in Multiple Linear Regression."
      ],
      "metadata": {
        "id": "gSvAMNLL7BOa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. What is polynomial regression?\n",
        "- Polynomial regression is a form of regression analysis in which the relationship between the independent variable\n",
        "ð‘‹\n",
        "X and the dependent variable\n",
        "ð‘Œ\n",
        "Y is modeled as an\n",
        "ð‘›\n",
        "n-th degree polynomial in\n",
        "ð‘‹\n",
        "X.\n",
        "\n",
        "- Although it models nonlinear relationships, it is linear in the parameters, so it is estimated using ordinary least squares.\n",
        "- Polynomial regression extends linear regression to model nonlinear relationships.\n",
        "\n",
        "- It remains a linear model in terms of estimation.\n",
        "\n",
        "- Powerful but must be used carefully to avoid overfitting and instability."
      ],
      "metadata": {
        "id": "rqbc_LY77SXa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "24. How does polynomial regression differ from linear regression?\n",
        "- | Aspect                | Linear Regression | Polynomial Regression      |\n",
        "| --------------------- | ----------------- | -------------------------- |\n",
        "| Shape                 | Straight line     | Curved line                |\n",
        "| Flexibility           | Low               | Higher (depends on degree) |\n",
        "| Captures nonlinearity | No                | Yes                        |\n",
        "\n",
        "- Linear regression fits straight lines; polynomial regression fits curves.\n",
        "\n",
        "- Polynomial regression increases flexibility at the cost of interpretability and stability.\n",
        "\n",
        "- Both use the same estimation framework but serve different modeling needs."
      ],
      "metadata": {
        "id": "GsbG2ooW7j7a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "25. When is polynomial regression used?\n",
        "- Polynomial regression is used when a linear regression model is too simple to capture the true relationship between variables, but a fully nonlinear model is unnecessary.\n",
        "- A linear model is inadequate due to curvature.\n",
        "\n",
        "- The relationship is smooth, nonlinear, and theoretically plausible.\n",
        "\n",
        "- Model complexity is kept under control through low-degree terms and validation.\n",
        "\n"
      ],
      "metadata": {
        "id": "QNtXGFv273Ba"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "26. What is the general equation for polynomial regression?\n",
        "- The general equation of a polynomial regression model of degree\n",
        "ð‘›\n",
        " with one independent variable is:\n",
        "- Y=Î²0â€‹+Î²1â€‹X+Î²2â€‹X2+Î²3â€‹X3+â‹¯+Î²nâ€‹Xn+Îµ\n",
        "  - Y: dependent (response) variable\n",
        "  - X: independent (predictor) variable\n",
        "  - Î²0 : intercept\n",
        "  - ð›½1,\n",
        "ð›½\n",
        "2\n",
        ",\n",
        "â€¦\n",
        ",\n",
        "ð›½\n",
        "ð‘›\tâ€‹\t: regression coefficients\n",
        "  \n",
        "  - ð‘‹2,ð‘‹3,â€¦,ð‘‹ð‘›: polynomial terms\n",
        "  - Îµ: error term\n",
        "- Although the model is nonlinear in\n",
        "ð‘‹\n",
        "X, it is linear in parameters, so it can be estimated using ordinary least squares.\n",
        "- Polynomial regression extends linear regression by adding powers of predictors.\n",
        "\n",
        "- The general form is a sum of polynomial terms plus an error term.\n",
        "\n",
        "- Model degree controls flexibility and risk of overfitting."
      ],
      "metadata": {
        "id": "7JU6Ck7X-QXs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "27. Can polynomial regression be applied to multiple variables?\n",
        "- Yes. Polynomial regression can be extended to multiple independent variables. This is commonly called multivariate (or multiple) polynomial regression.\n",
        "- Polynomial regression can be applied to multiple variables.\n",
        "\n",
        "- It includes powers and interaction terms of predictors.\n",
        "\n",
        "- It is powerful but must be used carefully to avoid overfitting and instability."
      ],
      "metadata": {
        "id": "1bEz19Zv_qoz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "28. What are the limitations of polynomial regression?\n",
        "- Polynomial regression is powerful for modeling smooth nonlinear relationships, but it has several important limitations.\n",
        "- | Limitation        | Consequence                      |\n",
        "| ----------------- | -------------------------------- |\n",
        "| Overfitting       | Poor predictions on new data     |\n",
        "| Extrapolation     | Unstable outside data range      |\n",
        "| Multicollinearity | Large SEs, unstable coefficients |\n",
        "| Interpretability  | Hard to explain effects          |\n",
        "| Rapid growth      | Needs large sample size          |\n",
        "\n",
        "\n",
        "- Polynomial regression is best suited for smooth, moderate nonlinear relationships within a limited data range. For complex or highly nonlinear patterns, alternatives like splines, GAMs, or tree-based models are often more appropriate."
      ],
      "metadata": {
        "id": "9QbNw8SK_3Ir"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "29. What methods can be used to evaluate model fit when selecting the degree of a polynomial?\n",
        "- Selecting the polynomial degree requires balancing fit quality and model complexity. The following methods are commonly used.\n",
        "- | Method           | Purpose                              |\n",
        "| ---------------- | ------------------------------------ |\n",
        "| Adjusted (R^2)   | Penalizes unnecessary complexity     |\n",
        "| Cross-validation | Evaluates out-of-sample performance  |\n",
        "| Residual plots   | Checks remaining structure           |\n",
        "| AIC / BIC        | Trade-off between fit and simplicity |\n",
        "| MSE / RMSE       | Measures prediction error            |\n",
        "\n",
        "- No single metric is sufficient. Robust degree selection relies on multiple evaluation methods to ensure good fit, stability, and generalization."
      ],
      "metadata": {
        "id": "hutQjQj9AETq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "30. Why is visualization important in polynomial regression?\n",
        "- Visualization is essential in polynomial regression because the model is flexible, nonlinear, and prone to overfitting, making numerical metrics alone insufficient.\n",
        "- Visualization in polynomial regression is important because it:\n",
        "  - Confirms the need for nonlinearity\n",
        "  - Prevents overfitting\n",
        "  - Guides degree selection\n",
        "  - Reveals instability and extrapolation issues\n",
        "  - Enhances interpretability\n",
        "\n",
        "- In polynomial regression, a model that looks wrong usually is, even if numerical metrics appear favorable."
      ],
      "metadata": {
        "id": "oV96L076AWg7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 31. How is polynomial regression implemented in Python?\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "\n",
        "# Example data\n",
        "X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)\n",
        "y = np.array([2, 4, 9, 16, 25])\n",
        "\n",
        "degree = 2  # quadratic\n",
        "\n",
        "model = Pipeline([\n",
        "    (\"poly\", PolynomialFeatures(degree=degree, include_bias=False)),\n",
        "    (\"linear\", LinearRegression())\n",
        "])\n",
        "\n",
        "model.fit(X, y)\n",
        "\n",
        "y_pred = model.predict(X)\n",
        "\n",
        "print(\"Intercept:\", model.named_steps[\"linear\"].intercept_)\n",
        "print(\"Coefficients:\", model.named_steps[\"linear\"].coef_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQMbxlzqA0mZ",
        "outputId": "65309b26-f67c-4461-f74f-b07530970684"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Intercept: 1.79999999999999\n",
            "Coefficients: [-1.05714286  1.14285714]\n"
          ]
        }
      ]
    }
  ]
}