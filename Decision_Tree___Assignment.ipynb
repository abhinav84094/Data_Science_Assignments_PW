{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. What is a Decision Tree, and how does it work in the context of\n",
        "classification?\n",
        "- A Decision Tree is a supervised machine learning algorithm used for classification and regression tasks.\n",
        "- In classification, it is used to predict a categorical output (e.g., Yes/No, Spam/Not Spam, Disease/No Disease).\n",
        "\n",
        "How Decision Tree Works in Classification\n",
        "- Step 1: Select the Best Feature (Splitting)\n",
        "  - The algorithm chooses the feature that best separates the data into classes.\n",
        "- Step 2: Split the Dataset\n",
        "  - The dataset is divided into subsets based on the selected feature.\n",
        "- Step 3: Repeat Recursively\n",
        "  - Again choose the best feature, Split the data\n",
        "- Step 4: Final Prediction"
      ],
      "metadata": {
        "id": "XXxAqClYAOJ8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Explain the concepts of Gini Impurity and Entropy as impurity measures.\n",
        "How do they impact the splits in a Decision Tree?\n",
        "- Both Gini Impurity and Entropy measure how “impure” (mixed) a node is.\n",
        "They help the tree decide where to split the data.\n",
        "- Entropy measures disorder using logarithms.\n",
        "- Gini measures probability of misclassification.\n",
        "- Both guide the tree to create pure child nodes.\n",
        "- The best split = one that reduces impurity the most."
      ],
      "metadata": {
        "id": "PW2SkPEtBU0v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What is the difference between Pre-Pruning and Post-Pruning in Decision\n",
        "Trees? Give one practical advantage of using each.\n",
        "- | Feature      | Pre-Pruning          | Post-Pruning              |\n",
        "| ------------ | -------------------- | ------------------------- |\n",
        "| When applied | During tree building | After full tree is built  |\n",
        "| Risk         | May underfit         | Less risk of underfitting |\n",
        "| Speed        | Faster               | Slower                    |\n",
        "| Computation  | Low                  | Higher                    |\n",
        "| Accuracy     | Sometimes lower      | Often better              |\n",
        "\n",
        "\n",
        "- Pre-Pruning prevents the tree from growing too complex early.\n",
        "  Advantage: Faster & computationally efficient.\n",
        "- Post-Pruning trims the tree after full growth.\n",
        " Advantage: Better generalization and accuracy in many cases."
      ],
      "metadata": {
        "id": "EtEYKjeeAOMu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What is Information Gain in Decision Trees, and why is it important for\n",
        "choosing the best split?\n",
        "- Information Gain (IG) measures how much uncertainty (entropy) decreases after splitting a dataset on a feature.\n",
        "\n",
        "In simple words:\n",
        "- Information Gain tells us how good a feature is at separating the data into classes.\n",
        "\n",
        "Why Is It Important?\n",
        "- In a Decision Tree: At every node:\n",
        "  - Try splitting on each feature\n",
        "  -Compute Information Gain\n",
        "  - Choose the feature with highest Information Gain\n",
        "\n",
        "This ensures the tree reduces impurity as much as possible at each step.\n",
        "- Information Gain measures reduction in entropy.\n",
        "- It is used to select the best feature at each node.\n",
        "- The feature with the highest Information Gain becomes the next decision node.\n",
        "- It helps the tree grow in the most informative direction."
      ],
      "metadata": {
        "id": "bFkvCU1_AOQF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What are some common real-world applications of Decision Trees, and\n",
        "what are their main advantages and limitations?\n",
        "- Decision Trees are widely used in real-world systems because they are simple yet powerful.\n",
        "- Common Real-World Applications of Decision Trees\n",
        "  - Healthcare (Disease Diagnosis)\n",
        "  - Finance & Banking (Credit Risk / Loan Approval)\n",
        "  - Marketing & Customer Analytics\n",
        "  - Manufacturing & Quality Control\n",
        "  - Education & HR\n",
        "\n",
        "Main Advantages of Decision Trees\n",
        "- Easy to Understand & Interpret\n",
        "  - Visual structure\n",
        "  - Human-readable rules\n",
        "  - No “black-box” behavior\n",
        "\n",
        "- Works with Different Data Types\n",
        "  - Numerical\n",
        "  - Categorical\n",
        "  - No need for feature scaling\n",
        "\n",
        "- Little Data Preparation Needed\n",
        "  - No normalization required\n",
        "  - Handles missing values (in some implementations)\n",
        "\n",
        "- Fast Inference\n",
        "  - Predictions are quick (just follow decision path)\n",
        "\n",
        "Main Limitations of Decision Trees\n",
        "- Overfitting\n",
        "  - Deep trees memorize training data\n",
        "  - Need pruning to control complexity\n",
        "\n",
        "- High Variance\n",
        "  - Small changes in data can create a different tree\n",
        "\n",
        "- Greedy Algorithm\n",
        "  - Chooses best split locally\n",
        "  - May not find global optimal tree\n",
        "\n",
        "- Bias Toward Features with Many Categories\n",
        "  - Information Gain may favor such features\n"
      ],
      "metadata": {
        "id": "JHd-qqc2EemQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  6:   Write a Python program to:\n",
        "# ● Load the Iris Dataset\n",
        "# ● Train a Decision Tree Classifier using the Gini criterion\n",
        "# ● Print the model’s accuracy and feature importances\n",
        "\n",
        "# Import required libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train Decision Tree Classifier using Gini criterion\n",
        "clf = DecisionTreeClassifier(criterion=\"gini\", random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Model Accuracy:\", accuracy)\n",
        "\n",
        "# Print feature importances\n",
        "print(\"\\nFeature Importances:\")\n",
        "for feature_name, importance in zip(iris.feature_names, clf.feature_importances_):\n",
        "    print(f\"{feature_name}: {importance:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BhJwImGUGFhl",
        "outputId": "b0a3d304-1bf3-4693-cc5c-cf0365b73f49"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.0\n",
            "\n",
            "Feature Importances:\n",
            "sepal length (cm): 0.0000\n",
            "sepal width (cm): 0.0167\n",
            "petal length (cm): 0.9061\n",
            "petal width (cm): 0.0772\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  7:  Write a Python program to:\n",
        "# ● Load the Iris Dataset\n",
        "# ● Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to\n",
        "# a fully-grown tree.\n",
        "\n",
        "# Import required libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# 2. Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 3. Train a fully-grown Decision Tree\n",
        "full_tree = DecisionTreeClassifier(random_state=42)\n",
        "full_tree.fit(X_train, y_train)\n",
        "\n",
        "# 4. Train a Decision Tree with max_depth = 3\n",
        "limited_tree = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "limited_tree.fit(X_train, y_train)\n",
        "\n",
        "# 5. Make predictions\n",
        "y_pred_full = full_tree.predict(X_test)\n",
        "y_pred_limited = limited_tree.predict(X_test)\n",
        "\n",
        "# 6. Calculate accuracies\n",
        "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "accuracy_limited = accuracy_score(y_test, y_pred_limited)\n",
        "\n",
        "# 7. Print results\n",
        "print(\"Fully-Grown Tree Accuracy:\", accuracy_full)\n",
        "print(\"Max Depth = 3 Tree Accuracy:\", accuracy_limited)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CgpWaj-fGtDX",
        "outputId": "13c14002-90c1-4a67-86be-da542dfad0ed"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fully-Grown Tree Accuracy: 1.0\n",
            "Max Depth = 3 Tree Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  8: Write a Python program to:\n",
        "# ● Load the Boston Housing Dataset\n",
        "# ● Train a Decision Tree Regressor\n",
        "# ● Print the Mean Squared Error (MSE) and feature importances\n",
        "\n",
        "\n",
        "# Import required libraries\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# 1. Load the Boston Housing dataset from OpenML\n",
        "boston = fetch_openml(name=\"boston\", version=1, as_frame=True)\n",
        "X = boston.data\n",
        "y = boston.target.astype(float)\n",
        "\n",
        "# 2. Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 3. Train a Decision Tree Regressor\n",
        "regressor = DecisionTreeRegressor(random_state=42)\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "# 4. Make predictions\n",
        "y_pred = regressor.predict(X_test)\n",
        "\n",
        "# 5. Calculate Mean Squared Error (MSE)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Mean Squared Error (MSE):\", mse)\n",
        "\n",
        "# 6. Print feature importances\n",
        "print(\"\\nFeature Importances:\")\n",
        "for feature_name, importance in zip(X.columns, regressor.feature_importances_):\n",
        "    print(f\"{feature_name}: {importance:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vtRWNz4G_lf",
        "outputId": "49022b38-e8c4-43c3-b151-5290a11d317e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (MSE): 10.416078431372549\n",
            "\n",
            "Feature Importances:\n",
            "CRIM: 0.0513\n",
            "ZN: 0.0034\n",
            "INDUS: 0.0058\n",
            "CHAS: 0.0000\n",
            "NOX: 0.0271\n",
            "RM: 0.6003\n",
            "AGE: 0.0136\n",
            "DIS: 0.0707\n",
            "RAD: 0.0019\n",
            "TAX: 0.0125\n",
            "PTRATIO: 0.0110\n",
            "B: 0.0090\n",
            "LSTAT: 0.1933\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  9: Write a Python program to:\n",
        "# ● Load the Iris Dataset\n",
        "# ● Tune the Decision Tree’s max_depth and min_samples_split using\n",
        "# GridSearchCV\n",
        "# ● Print the best parameters and the resulting model accuracy\n",
        "\n",
        "# Import required libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# 2. Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 3. Define the Decision Tree model\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# 4. Define the parameter grid\n",
        "param_grid = {\n",
        "    \"max_depth\": [None, 2, 3, 4, 5],\n",
        "    \"min_samples_split\": [2, 5, 10]\n",
        "}\n",
        "\n",
        "# 5. Apply GridSearchCV\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=dt,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,\n",
        "    scoring=\"accuracy\",\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# 6. Get the best model\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# 7. Make predictions using the best model\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# 8. Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# 9. Print results\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"Model Accuracy with Best Parameters:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "698cvSAYHMJ4",
        "outputId": "53b88ca1-a345-4f4b-84d3-e2947b70a1d1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': None, 'min_samples_split': 2}\n",
            "Model Accuracy with Best Parameters: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10: Imagine you’re working as a data scientist for a healthcare company that\n",
        "wants to predict whether a patient has a certain disease. You have a large dataset with\n",
        "mixed data types and some missing values.\n",
        "Explain the step-by-step process you would follow to:\n",
        "- Handle the missing values\n",
        "- Encode the categorical features\n",
        "- Train a Decision Tree model\n",
        "- Tune its hyperparameters\n",
        "- Evaluate its performance\n",
        "And describe what business value this model could provide in the real-world\n",
        "setting.\n",
        "\n",
        "\n",
        "Below is a structured, real-world workflow I would follow as a data scientist in a healthcare setting.\n",
        "\n",
        "1. Understand the Problem and Data\n",
        "\n",
        "Before modeling:\n",
        "\n",
        "* Define the target variable (Disease: Yes/No).\n",
        "* Identify feature types:\n",
        "\n",
        "  * Numerical (age, blood pressure, cholesterol)\n",
        "  * Categorical (gender, symptoms, medical history)\n",
        "* Check class imbalance.\n",
        "* Explore missing values and data distribution.\n",
        "\n",
        "This ensures the modeling approach aligns with the medical objective.\n",
        "\n",
        "---\n",
        "\n",
        "2. Handle Missing Values\n",
        "\n",
        "Step 1: Analyze Missingness\n",
        "\n",
        "* Identify percentage of missing values per column.\n",
        "* Check if missingness is random or systematic.\n",
        "\n",
        "Step 2: Apply Appropriate Strategy\n",
        "\n",
        "For Numerical Features:\n",
        "\n",
        "* If small percentage missing → Impute using median (robust to outliers).\n",
        "* If clinically meaningful → Use domain-specific imputation.\n",
        "* Optionally add a “missing indicator” column.\n",
        "\n",
        "For Categorical Features:\n",
        "\n",
        "* Replace missing with:\n",
        "\n",
        "  * Most frequent category, or\n",
        "  * A new category like \"Unknown\".\n",
        "\n",
        "If a feature has extremely high missing rate (e.g., >50%), consider dropping it after consulting domain experts.\n",
        "\n",
        "Why this matters:\n",
        "Healthcare data often has missing lab tests; improper handling may bias predictions.\n",
        "\n",
        "---\n",
        "\n",
        "3. Encode Categorical Features\n",
        "\n",
        "Decision Trees do not require scaling but cannot handle raw text categories.\n",
        "\n",
        "If using scikit-learn:\n",
        "\n",
        "* Use One-Hot Encoding for nominal categories (e.g., blood type).\n",
        "* Use Ordinal Encoding if categories have meaningful order (e.g., severity: mild, moderate, severe).\n",
        "\n",
        "Pipeline example:\n",
        "\n",
        "* ColumnTransformer:\n",
        "\n",
        "  * Numerical → Imputer\n",
        "  * Categorical → Imputer + OneHotEncoder\n",
        "\n",
        "This ensures consistent preprocessing during training and prediction.\n",
        "\n",
        "---\n",
        "\n",
        "4. Train the Decision Tree Model\n",
        "\n",
        "Steps:\n",
        "\n",
        "1. Split data into:\n",
        "\n",
        "   * Training set (80%)\n",
        "   * Test set (20%)\n",
        "\n",
        "2. Build preprocessing + model pipeline:\n",
        "\n",
        "   * Imputation\n",
        "   * Encoding\n",
        "   * DecisionTreeClassifier\n",
        "\n",
        "3. Fit the model on training data.\n",
        "\n",
        "Decision Trees are suitable here because:\n",
        "\n",
        "* They handle mixed data types.\n",
        "* They are interpretable.\n",
        "* No scaling required.\n",
        "\n",
        "---\n",
        "\n",
        "5. Tune Hyperparameters\n",
        "\n",
        "To prevent overfitting, tune:\n",
        "\n",
        "* max_depth\n",
        "* min_samples_split\n",
        "* min_samples_leaf\n",
        "* max_features\n",
        "* criterion (gini or entropy)\n",
        "\n",
        "Use GridSearchCV or RandomizedSearchCV with cross-validation (e.g., 5-fold CV).\n",
        "\n",
        "Example approach:\n",
        "\n",
        "* Define parameter grid\n",
        "* Use cross-validation\n",
        "* Select best model based on accuracy, F1-score, or ROC-AUC\n",
        "\n",
        "In healthcare, F1-score or Recall is often more important than accuracy.\n",
        "\n",
        "---\n",
        "\n",
        "6. Evaluate Model Performance\n",
        "\n",
        "Use multiple evaluation metrics:\n",
        "\n",
        "For Classification:\n",
        "\n",
        "* Accuracy\n",
        "* Precision\n",
        "* Recall (very important for disease detection)\n",
        "* F1-score\n",
        "* ROC-AUC\n",
        "* Confusion Matrix\n",
        "\n",
        "Why Recall matters:\n",
        "\n",
        "* False negatives (missed disease cases) can be dangerous.\n",
        "* High recall ensures fewer sick patients are missed.\n",
        "\n",
        "Also check:\n",
        "\n",
        "* Overfitting (train vs test performance)\n",
        "* Feature importance for interpretability\n",
        "\n",
        "---\n",
        "\n",
        "7. Model Interpretation\n",
        "\n",
        "Decision Trees are interpretable:\n",
        "\n",
        "* Extract decision rules\n",
        "* Visualize tree\n",
        "* Analyze feature importances\n",
        "\n",
        "Doctors can understand rules like:\n",
        "\n",
        "\"If glucose > X and age > Y, then high risk.\"\n",
        "\n",
        "This transparency is crucial in healthcare applications.\n",
        "\n",
        "---\n",
        "\n",
        "8. Business Value in Real-World Healthcare\n",
        "\n",
        "9. Early Disease Detection -\n",
        "   Helps identify high-risk patients before symptoms worsen.\n",
        "\n",
        "10. Improved Clinical Decision Support -\n",
        "    Assists doctors with data-driven recommendations.\n",
        "\n",
        "11. Cost Reduction -\n",
        "    Early diagnosis reduces hospitalization costs.\n",
        "\n",
        "12. Resource Optimization -\n",
        "    Hospitals can prioritize high-risk patients.\n",
        "\n",
        "13. Risk Stratification -\n",
        "    Enables preventive care programs.\n",
        "\n",
        "14. Better Patient Outcomes -\n",
        "    Faster intervention improves survival rates.\n",
        "\n",
        "---\n",
        "\n",
        "Final Summary\n",
        "\n",
        "The complete workflow would be:\n",
        "\n",
        "1. Understand data and objective.\n",
        "2. Handle missing values carefully.\n",
        "3. Encode categorical variables properly.\n",
        "4. Train Decision Tree using a pipeline.\n",
        "5. Tune hyperparameters with cross-validation.\n",
        "6. Evaluate using clinically relevant metrics.\n",
        "7. Interpret model and deploy responsibly.\n",
        "\n",
        "In a healthcare setting, the true value of this model is not just accuracy but its ability to provide reliable, interpretable, and actionable insights that support better medical decisions.\n"
      ],
      "metadata": {
        "id": "R6qawJ1KHVDF"
      }
    }
  ]
}